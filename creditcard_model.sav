# -*- coding: utf-8 -*-
"""Copy of Online Credit Card Fraud Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pToqmP9lUyUnYgwciMthDXl2RPRP5fjA

Importing the Dependencies
"""

import numpy as np
 import pandas as pd
 from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LogisticRegression
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.metrics import accuracy_score



# loading the dataset(csv file) to a Pandas DataFrame
credit_card_data = pd.read_csv('/content/creditcard.csv')

# first 5 rows of the dataset
credit_card_data.head()

#These values(V1-V28) are some features particular details about the transaction..but cant give us details about the credit card......these dataset has converted all the features through principle components anaylsis method and convert all features to numerical and will use numerical for analysis and prediction
# class shows whether the particular transaction is valid or not.. 0 shows legit transaction and 1 shows fraudlent transaction
# the head function gives the 1st 5 rows and tail gives the last 5 rows
# the time is in seconds..

credit_card_data.tail()

# lets get some dataset information
credit_card_data.info()

"""#it tells us the no. of enteries, no. of columns, datatypes of each column, and  no. of values present 

"""

# checking the no. of missing values in each column
credit_card_data.isnull().sum()

#shows no missing values
 # if we have missing value...we can handle it by the method called as simpucation
 #if have then have to do some more processing to convert them to meaningful values

#how these class is distributed........Distribution of legit transaction and fraudulent transaction
credit_card_data['Class'].value_counts()

#for normal transaction(0) we have 20327 datapoints and for fraudulent transaction(1) we have 86 datapoints
#this is unbalanced dataset

# spearating the data for analysis
legit = credit_card_data[credit_card_data.Class == 0]
fraud = credit_card_data[credit_card_data.Class == 1]

print(legit.shape)
print(fraud.shape)

#if label is 0 all the data in that row will be stored in the legit variable and all the values 1 will be stored in fraud variable with label 
# print shape of the two variables(legit, fraud)...in the form of (no. of normal transactions, columns) and (no. of fraudulent transaction, columns).
#credit_card_data is a frame

#statistical measurement of the data......this will take amount column...how much money is taken by transaction
legit.Amount.describe()

#some statistical measures are done on amount column
#count value tells us about the no. of data point in legit amount, std means standard deviation
#25% , 50%, 75% these are percentile value...not percentage....it means..25% of the transaction amount is less than the 5 dollars

# for fraudulent data
fraud.Amount.describe()

#mean value in fraudulent is quiet greater than legit

#compare the values for both transactions
#compare mean value of all columns for these two different classes
credit_card_data.groupby('Class').mean()

#by this it predicts if transaction is legit or fraudulent
#wide difference between the mean of fraud and normal

# dealing with the unbalanced data
#method called as under-sampling

"""Under Sampling"""

#Build a sample dataset containing a similar distribution of legit transaction and the fraudulent transaction

"""No. of fraudulent transactions-> 3
here we are taking randome sample from 86 ..into legit variable
from 5844 normal transactions we are taking 3 random transactions and concatenating it with fraudulent transaction i.e also 3


"""

legit_sample = legit.sample(n=3)

"""concatenating two dataframe ...i.e 1st legit sample and fraudulent ...both contains 3 values """

new_dataset =  pd.concat([legit_sample, fraud], axis=0)
# axis=0 ...so dataset should be added one by one ......here we have legit sample so all the 3 values will be added below this legit sample
# if axis=1....all the values will be added column

new_dataset.head()
#these are the random values of legit transaction

new_dataset.tail()

new_dataset['Class'].value_counts()
# value counts of no. of fraud and legit.....group by will give the values of two transaction based on the mean

new_dataset.groupby('Class').mean()
# here the mean in new dataset

"""Splitting the data into features and targets.i.e either 0 or y and store them in two variables x and y"""

X = new_dataset.drop(columns='Class', axis=1)
#axis=0 represents row and 1 represents column
Y = new_dataset['Class']

print(X)
#it wont print class column ...previously was having 31 columns and now have 30 columns

print(Y)

"""split the data into training data & Testing data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)
#all the features will be in X and labels in Y
# features will be splitted in training data X and corresponding labels of corresponding data in Y
#0.2 means 20% of data 
# stratify= Y ...based on the Y...if not distribution of X and Y will be random 
#random means how we want to split the data

print(X.shape, X_train.shape, X_test.shape)

"""Model Training
Logistic Regression here we use.... generally we use for binary classification
already imported logistic regression from that sklearn.linear_model import LogisitcRegression 
"""

model = LogisticRegression()
# variable model this paranthesis is for we are loading 1 instance of logistic regression model in this variable model

# training the logistic regression model with training data
# fit function is used to fit the training data in logistic model
model.fit(X_train, Y_train)



"""Model Evaluation...based on accuracy score"""

# accuracy on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy on Training data: ', training_data_accuracy)
# accuracy scoreis almost 100%...means our predictions are right

# accuracy score on test data 
# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score on Test Data : ', test_data_accuracy)
# it is almost 50%

#if accuracy score of test data and accuracy score on training data have large difference ..then our model has over fitting or under fitting with training data

